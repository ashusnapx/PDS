{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Min-Max Scaling:**\n",
    "\n",
    "Min-Max scaling, also known as feature scaling or normalization, is a data preprocessing technique used to transform numerical features in a way that they are scaled between a specified range, typically 0 and 1. This technique is applied to ensure that all features have the same scale, which can be important for certain algorithms that are sensitive to the scale of features, such as distance-based algorithms.\n",
    "\n",
    "The formula for Min-Max scaling is as follows:\n",
    "\n",
    "```\n",
    "X_scaled = (X - X_min) / (X_max - X_min)\n",
    "```\n",
    "\n",
    "Here, `X` is the original feature value, `X_min` is the minimum value of the feature, and `X_max` is the maximum value of the feature.\n",
    "\n",
    "**How Min-Max Scaling is Used:**\n",
    "\n",
    "Min-Max scaling is used to bring all numerical features to a common scale, making them comparable and avoiding any bias due to differing scales. This is particularly important for machine learning algorithms like k-nearest neighbors and neural networks, where distances or weight updates are influenced by the scale of the features.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's say we have a dataset with a feature \"Age\" that ranges from 0 to 100 and a feature \"Income\" that ranges from 20000 to 100000. To use Min-Max scaling, we would transform these features to have values between 0 and 1.\n",
    "\n",
    "Original data:\n",
    "```\n",
    "Age: [25, 50, 30, 70, 40]\n",
    "Income: [30000, 60000, 35000, 80000, 45000]\n",
    "```\n",
    "\n",
    "Min-Max scaled data (assuming Age ranges from 0 to 100 and Income ranges from 20000 to 100000):\n",
    "```\n",
    "Age: [0.25, 0.50, 0.30, 0.70, 0.40]\n",
    "Income: [0.20, 0.55, 0.25, 0.90, 0.35]\n",
    "```\n",
    "\n",
    "In this example, both \"Age\" and \"Income\" are scaled between 0 and 1 using Min-Max scaling. This ensures that both features have the same scale and avoids potential issues in algorithms that rely on feature scales.\n",
    "\n",
    "Min-Max scaling, while useful for bringing features to a common scale, can be sensitive to outliers. Extreme values in the data can disproportionately affect the scaling. In such cases, other scaling techniques like Z-score scaling (standardization) might be more appropriate.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unit Vector Scaling:**\n",
    "\n",
    "Unit Vector scaling, also known as vector normalization, is a feature scaling technique that scales the data in such a way that each data point (vector) has a length of 1. This technique is particularly useful when the direction of the data vectors matters more than their magnitudes. It's often used in machine learning algorithms that involve distance calculations or angle-based measurements.\n",
    "\n",
    "To perform unit vector scaling, each data point is divided by its own Euclidean norm (L2 norm), resulting in vectors with a length of 1 while maintaining their original directions.\n",
    "\n",
    "**Difference Between Unit Vector Scaling and Min-Max Scaling:**\n",
    "\n",
    "1. **Objective:**\n",
    "   - Unit Vector Scaling: Focuses on preserving the direction of data vectors.\n",
    "   - Min-Max Scaling: Focuses on bringing data features to a common scale between a specified range.\n",
    "\n",
    "2. **Scale Range:**\n",
    "   - Unit Vector Scaling: Does not target a specific scale range; only the direction matters.\n",
    "   - Min-Max Scaling: Scales features between a specified range (e.g., 0 and 1).\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider a dataset with two features: \"Temperature\" and \"Humidity.\" Each data point in this example represents a weather condition, with \"Temperature\" and \"Humidity\" being the two features. Unit Vector scaling is applied to make the direction of each data point's vector consistent:\n",
    "\n",
    "Original data:\n",
    "```\n",
    "Temperature: [30, 20, 25, 35, 28]\n",
    "Humidity: [60, 40, 50, 70, 55]\n",
    "```\n",
    "\n",
    "Unit Vector scaled data:\n",
    "```\n",
    "Temperature: [0.447, 0.447, 0.447, 0.447, 0.447]\n",
    "Humidity: [0.763, 0.763, 0.763, 0.763, 0.763]\n",
    "```\n",
    "\n",
    "In this example, the direction of each data point's vector is preserved while scaling each vector to have a length of 1 using Unit Vector scaling.\n",
    "\n",
    "It's important to note that Unit Vector scaling does not necessarily bring the data to a common scale like Min-Max scaling does. Instead, it ensures that the direction of vectors is consistent, which is valuable when dealing with algorithms that use vector angles or magnitudes.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA):**\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional representation while preserving as much of the original data's variability as possible. It does this by identifying the principal components, which are new orthogonal (uncorrelated) axes in the data space. These principal components are linear combinations of the original features, and they capture the most important patterns and variations in the data.\n",
    "\n",
    "PCA is commonly used for:\n",
    "- Reducing the number of features to improve computational efficiency.\n",
    "- Visualizing high-dimensional data in lower-dimensional spaces.\n",
    "- Removing noise and redundancy from the data.\n",
    "\n",
    "**How PCA is Used in Dimensionality Reduction:**\n",
    "\n",
    "1. **Calculate the Covariance Matrix:** The first step in PCA is to calculate the covariance matrix of the original data, which represents the relationships and interactions between the features.\n",
    "\n",
    "2. **Compute Eigenvectors and Eigenvalues:** Next, the eigenvectors and eigenvalues of the covariance matrix are computed. Eigenvectors are the principal components, and eigenvalues indicate the variance explained by each eigenvector.\n",
    "\n",
    "3. **Select Principal Components:** Principal components are selected based on their corresponding eigenvalues. The components with higher eigenvalues capture more variance in the data and are selected for the new lower-dimensional space.\n",
    "\n",
    "4. **Project Data:** The original data is projected onto the new lower-dimensional space defined by the selected principal components.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider a dataset with two features: \"Height\" and \"Weight.\" Each data point represents an individual's height and weight. We'll use PCA to reduce the data to one dimension (a single principal component).\n",
    "\n",
    "Original data:\n",
    "```\n",
    "Height: [160, 165, 155, 170, 175]\n",
    "Weight: [60, 68, 54, 72, 70]\n",
    "```\n",
    "\n",
    "After performing PCA, the first principal component is found to be:\n",
    "```\n",
    "Principal Component 1: [0.707, 0.707]\n",
    "```\n",
    "\n",
    "Now, we project the original data onto this single principal component:\n",
    "```\n",
    "Projected Data: [113.14, 116.78, 104.84, 128.85, 133.57]\n",
    "```\n",
    "\n",
    "In this example, the two-dimensional data has been reduced to a single dimension using PCA. The first principal component captures the most significant variation in the data, allowing us to represent the original data with fewer dimensions. The projected data represents the new lower-dimensional representation that preserves as much of the original variability as possible.\n",
    "\n",
    "Keep in mind that while PCA is powerful for reducing dimensionality, it might not always be suitable for all datasets, and careful consideration should be given to the amount of variance retained and the interpretability of the reduced dimensions.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Relationship Between PCA and Feature Extraction:**\n",
    "\n",
    "PCA (Principal Component Analysis) is closely related to feature extraction in that it is a technique used to transform high-dimensional data into a lower-dimensional representation. In the context of feature extraction, PCA is a method for creating new features (principal components) that capture the most important patterns and variations in the data. These principal components are linear combinations of the original features and are designed to reduce dimensionality while preserving the most relevant information.\n",
    "\n",
    "**Using PCA for Feature Extraction:**\n",
    "\n",
    "1. **Calculate Covariance Matrix:** Similar to PCA for dimensionality reduction, the first step is to calculate the covariance matrix of the original data.\n",
    "\n",
    "2. **Compute Eigenvectors and Eigenvalues:** The eigenvectors and eigenvalues of the covariance matrix are computed. Eigenvectors represent the directions in the data space along which the data varies the most, and eigenvalues indicate the amount of variance explained by each eigenvector.\n",
    "\n",
    "3. **Select Principal Components:** Instead of selecting a fixed number of principal components as in dimensionality reduction, in feature extraction, you might select a subset of the principal components that capture a desired amount of variance. These components become the new extracted features.\n",
    "\n",
    "4. **Transform Data:** The original data is then transformed into the space defined by the selected principal components, resulting in a lower-dimensional representation with the new extracted features.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider a dataset with three features: \"Income,\" \"Age,\" and \"Education.\" We want to perform feature extraction using PCA to create two new features that capture the most important patterns in the data.\n",
    "\n",
    "Original data:\n",
    "```\n",
    "Income: [50000, 75000, 60000, 80000, 55000]\n",
    "Age: [30, 40, 35, 45, 28]\n",
    "Education: [12, 16, 14, 18, 10]\n",
    "```\n",
    "\n",
    "After performing PCA for feature extraction, let's say we choose to extract two principal components:\n",
    "\n",
    "```\n",
    "Principal Component 1: [0.556, 0.631, 0.575, 0.696, 0.502]\n",
    "Principal Component 2: [0.725, -0.672, -0.189, 0.144, -0.047]\n",
    "```\n",
    "\n",
    "Now, our new extracted features are the projections of the original data onto these principal components:\n",
    "\n",
    "```\n",
    "Feature 1 (extracted): [78.7, 105.8, 83.5, 118.0, 67.7]\n",
    "Feature 2 (extracted): [0.164, -0.157, -0.044, 0.034, -0.011]\n",
    "```\n",
    "\n",
    "In this example, PCA is used for feature extraction to create two new features that capture the main patterns in the original data. These extracted features represent combinations of the original features that provide a lower-dimensional representation while retaining relevant information.\n",
    "\n",
    "Keep in mind that feature extraction using PCA can help in reducing dimensionality and highlighting key patterns, but it might result in features that are not directly interpretable in the original context.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-Max scaling could be used to preprocess the data for building a recommendation system for a food delivery service, using the example of Uber Eats.\n",
    "\n",
    "**Dataset Features:**\n",
    "- Price (ranging from $ to $$$): Indicates the cost of food items.\n",
    "- Rating (ranging from 1 to 5): Represents the average customer rating.\n",
    "- Delivery Time (in minutes): Indicates the estimated time for delivery.\n",
    "\n",
    "**Min-Max Scaling:**\n",
    "\n",
    "Min-Max scaling is used to transform the data in a way that all features have values between 0 and 1, based on their original ranges.\n",
    "\n",
    "**Steps to Use Min-Max Scaling:**\n",
    "\n",
    "1. **Identify the Ranges:**\n",
    "   Determine the original ranges for each feature. For example:\n",
    "   - Price: $ (min) to $$$ (max)\n",
    "   - Rating: 1 (min) to 5 (max)\n",
    "   - Delivery Time: Minimum and maximum delivery times.\n",
    "\n",
    "2. **Apply Min-Max Scaling Formula:**\n",
    "   The formula to scale a feature using Min-Max scaling is:\n",
    "   \n",
    "   ```\n",
    "   X_scaled = (X - X_min) / (X_max - X_min)\n",
    "   ```\n",
    "\n",
    "   Where `X` is the original feature value, `X_min` is the minimum value of the feature, and `X_max` is the maximum value of the feature.\n",
    "\n",
    "3. **Normalize the Data:**\n",
    "   Apply the Min-Max scaling formula to each feature to transform the data. This will result in each feature having values between 0 and 1.\n",
    "\n",
    "**Example Using Uber Eats:**\n",
    "\n",
    "Let's assume we have the following data for three different food items:\n",
    "\n",
    "```\n",
    "Item 1:\n",
    "- Price: $$\n",
    "- Rating: 4.5\n",
    "- Delivery Time: 30 minutes\n",
    "\n",
    "Item 2:\n",
    "- Price: $\n",
    "- Rating: 3.7\n",
    "- Delivery Time: 45 minutes\n",
    "\n",
    "Item 3:\n",
    "- Price: $$$\n",
    "- Rating: 4.9\n",
    "- Delivery Time: 20 minutes\n",
    "```\n",
    "\n",
    "Now, we apply Min-Max scaling to each feature:\n",
    "\n",
    "- Price (MinMax scaled):\n",
    "  - Item 1: (2 - 1) / (3 - 1) = 0.5\n",
    "  - Item 2: (1 - 1) / (3 - 1) = 0\n",
    "  - Item 3: (3 - 1) / (3 - 1) = 1\n",
    "\n",
    "- Rating (MinMax scaled):\n",
    "  - Item 1: (4.5 - 1) / (5 - 1) = 0.875\n",
    "  - Item 2: (3.7 - 1) / (5 - 1) = 0.675\n",
    "  - Item 3: (4.9 - 1) / (5 - 1) = 0.975\n",
    "\n",
    "- Delivery Time (MinMax scaled):\n",
    "  - Item 1: (30 - 20) / (45 - 20) = 0.333\n",
    "  - Item 2: (45 - 20) / (45 - 20) = 1\n",
    "  - Item 3: (20 - 20) / (45 - 20) = 0\n",
    "\n",
    "After applying Min-Max scaling, the data is transformed to have values between 0 and 1 for each feature. This ensures that all features are on a comparable scale and prevents one feature from dominating the others when used in the recommendation system algorithm.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely, let's work through a small example of using PCA for dimensionality reduction in the context of stock price prediction. For simplicity, we'll consider a simplified dataset with three features: Revenue, Profit, and Market Trend Index for five different companies over a span of four quarters.\n",
    "\n",
    "Here's the dataset:\n",
    "```\n",
    "| Company | Quarter 1 Revenue | Quarter 1 Profit | Quarter 1 Index |\n",
    "|---------|------------------|------------------|-----------------|\n",
    "| A       | 100              | 10               | 30              |\n",
    "| B       | 150              | 15               | 35              |\n",
    "| C       | 80               | 5                | 25              |\n",
    "| D       | 200              | 20               | 40              |\n",
    "| E       | 120              | 12               | 32              |\n",
    "```\n",
    "\n",
    "**Step 1: Standardize the Data:**\n",
    "We'll calculate the mean and standard deviation for each feature and use them to standardize the data.\n",
    "\n",
    "```\n",
    "Mean(Revenue) = (100 + 150 + 80 + 200 + 120) / 5 = 130\n",
    "StdDev(Revenue) = sqrt((sum((x - mean)^2) / (n - 1))) = 42.55\n",
    "\n",
    "Mean(Profit) = (10 + 15 + 5 + 20 + 12) / 5 = 12.4\n",
    "StdDev(Profit) = 5.06\n",
    "\n",
    "Mean(Index) = (30 + 35 + 25 + 40 + 32) / 5 = 32.4\n",
    "StdDev(Index) = 5.33\n",
    "```\n",
    "\n",
    "Standardized data:\n",
    "```\n",
    "| Company | Quarter 1 Revenue | Quarter 1 Profit | Quarter 1 Index |\n",
    "|---------|-------------------|------------------|-----------------|\n",
    "| A       | -0.65             | -0.64            | -0.60           |\n",
    "| B       | 0.26              | 0.64             | 0.60            |\n",
    "| C       | -1.26             | -1.28            | -1.80           |\n",
    "| D       | 1.30              | 1.28             | 1.80            |\n",
    "| E       | -0.66             | -0.00            | -0.00           |\n",
    "```\n",
    "\n",
    "**Step 2: Calculate Covariance Matrix:**\n",
    "The covariance matrix of the standardized data is calculated as follows:\n",
    "\n",
    "```\n",
    "Covariance Matrix = \n",
    "| 0.99   0.97   0.97 |\n",
    "| 0.97   0.97   0.96 |\n",
    "| 0.97   0.96   0.99 |\n",
    "```\n",
    "\n",
    "**Step 3: Compute Eigenvectors and Eigenvalues:**\n",
    "Calculate the eigenvectors and eigenvalues of the covariance matrix. The eigenvalues are 2.90, 0.03, and 0.02, and the corresponding eigenvectors are [-0.62, -0.67, -0.41], [-0.66, 0.74, -0.07], and [-0.43, -0.03, 0.90].\n",
    "\n",
    "**Step 4: Sort Eigenvectors by Eigenvalues:**\n",
    "Sort the eigenvectors in descending order based on their corresponding eigenvalues.\n",
    "\n",
    "```\n",
    "Eigenvalues: [2.90, 0.03, 0.02]\n",
    "Eigenvectors: \n",
    "[-0.62, -0.67, -0.41]\n",
    "[-0.66, 0.74, -0.07]\n",
    "[-0.43, -0.03, 0.90]\n",
    "```\n",
    "\n",
    "**Step 5: Select Principal Components:**\n",
    "In this example, let's say we want to retain the two principal components that capture the most variance.\n",
    "\n",
    "Selected eigenvectors: [-0.62, -0.67, -0.41] and [-0.66, 0.74, -0.07].\n",
    "\n",
    "**Step 6: Project Data:**\n",
    "Project the original standardized data onto the selected principal components.\n",
    "\n",
    "Projected data:\n",
    "```\n",
    "| Company | Principal Component 1 | Principal Component 2 |\n",
    "|---------|-----------------------|-----------------------|\n",
    "| A       | 1.61                  | 0.48                  |\n",
    "| B       | -0.84                 | 0.04                  |\n",
    "| C       | 1.42                  | 0.48                  |\n",
    "| D       | -2.22                 | 0.13                  |\n",
    "| E       | 0.02                  | -0.61                 |\n",
    "```\n",
    "\n",
    "In this example, we have successfully reduced the dimensionality of the original dataset from three features to two principal components. These principal components capture the most significant patterns and variations in the data. The projected data can now be used for building a stock price prediction model in a lower-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform Min-Max scaling and transform the values to a range of -1 to 1, follow these steps:\n",
    "\n",
    "**Original Data:**\n",
    "```\n",
    "[1, 5, 10, 15, 20]\n",
    "```\n",
    "\n",
    "**Step 1: Calculate Min and Max:**\n",
    "Calculate the minimum and maximum values in the original dataset.\n",
    "```\n",
    "Min: 1\n",
    "Max: 20\n",
    "```\n",
    "\n",
    "**Step 2: Apply Min-Max Scaling Formula:**\n",
    "Apply the Min-Max scaling formula to each data point:\n",
    "```\n",
    "X_scaled = (X - X_min) / (X_max - X_min)\n",
    "```\n",
    "\n",
    "**Step 3: Transform Data:**\n",
    "Calculate the scaled values using the formula and transform each data point.\n",
    "\n",
    "Scaled Data:\n",
    "```\n",
    "Scaled(1) = (1 - 1) / (20 - 1) = 0\n",
    "Scaled(5) = (5 - 1) / (20 - 1) = 0.25\n",
    "Scaled(10) = (10 - 1) / (20 - 1) = 0.45\n",
    "Scaled(15) = (15 - 1) / (20 - 1) = 0.7\n",
    "Scaled(20) = (20 - 1) / (20 - 1) = 1\n",
    "```\n",
    "\n",
    "**Step 4: Transform to Range of -1 to 1:**\n",
    "Transform the scaled values to the desired range of -1 to 1.\n",
    "Transformed(X) = (Scaled(X) * (max_range - min_range)) + min_range\n",
    "```\n",
    "Transformed(1) = (Scaled(1) * 2) - 1 = -1\n",
    "Transformed(5) = (Scaled(5) * 2) - 1 = -0.5\n",
    "Transformed(10) = (Scaled(10) * 2) - 1 = -0.1\n",
    "Transformed(15) = (Scaled(15) * 2) - 1 = 0.4\n",
    "Transformed(20) = (Scaled(20) * 2) - 1 = 1\n",
    "```\n",
    "\n",
    "So, after performing Min-Max scaling and transforming the values to a range of -1 to 1, the transformed values will be:\n",
    "```\n",
    "[-1, -0.5, -0.1, 0.4, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing feature extraction using PCA (Principal Component Analysis), the number of principal components to retain is a crucial decision. It depends on the trade-off between retaining the variance in the data and reducing dimensionality. To decide the number of principal components, you typically consider the cumulative explained variance and the application's requirements.\n",
    "\n",
    "Here's a general approach to help you decide the number of principal components to retain:\n",
    "\n",
    "**Step 1: Standardize the Data**\n",
    "Before applying PCA, standardize the data to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "**Step 2: Calculate Covariance Matrix and Eigenvalues**\n",
    "Compute the covariance matrix of the standardized data and calculate its eigenvalues.\n",
    "\n",
    "**Step 3: Sort Eigenvalues**\n",
    "Sort the eigenvalues in descending order.\n",
    "\n",
    "**Step 4: Calculate Cumulative Explained Variance**\n",
    "For each eigenvalue, calculate the explained variance ratio (eigenvalue / total sum of eigenvalues). Calculate the cumulative explained variance by summing up the explained variance ratios in descending order.\n",
    "\n",
    "**Step 5: Choose the Number of Principal Components**\n",
    "Choose the number of principal components that collectively explain a significant amount of variance. A common guideline is to retain enough principal components to cover a high percentage (e.g., 95% or more) of the total variance.\n",
    "\n",
    "Now, let's apply this approach to your dataset containing the features: height, weight, age, gender, and blood pressure.\n",
    "\n",
    "- **height, weight, age, and blood pressure:** Numerical features\n",
    "- **gender:** Categorical feature (may need encoding before PCA)\n",
    "\n",
    "For PCA, it's important to remember that categorical features like \"gender\" need to be transformed into numerical values before applying PCA.\n",
    "\n",
    "Once you've prepared the data, you can calculate the eigenvalues, explained variance ratios, and cumulative explained variance. Based on the cumulative explained variance, you can decide how many principal components to retain.\n",
    "\n",
    "Remember that the goal is to find a balance between reducing dimensionality while retaining enough variance to capture important patterns in the data. The specific number of principal components to retain can vary depending on the dataset, application, and the level of interpretability required.\n",
    "\n",
    "If the dataset has strong correlations between features, fewer principal components might be needed to capture most of the variance. If the features are relatively independent, you might need more principal components to explain the variance.\n",
    "\n",
    "In practice, you could create a scree plot (plot of eigenvalues) or a cumulative explained variance plot to visualize the trade-off between the number of principal components and the cumulative variance explained.\n",
    "\n",
    "Finally, it's recommended to experiment with different numbers of principal components and evaluate how well the reduced data performs in downstream tasks (e.g., machine learning models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
